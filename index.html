<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pokémon Fusion Art Generator - CS 566 Project</title>
    <style>
        :root {
            --primary-color: #EE1515; /* Pokémon Red */
            --secondary-color: #222224; /* Dark Gray */
            --accent-color: #3B4CCA; /* Pokemon Blue */
            --bg-color: #f8f9fa;
            --text-color: #333;
            --sidebar-width: 280px;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            background-color: var(--bg-color);
            display: flex;
        }

        /* Sidebar Navigation */
        nav {
            width: var(--sidebar-width);
            background-color: var(--secondary-color);
            color: white;
            height: 100vh;
            position: fixed;
            padding: 2rem 1rem;
            box-sizing: border-box;
            overflow-y: auto;
        }

        nav h3 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--primary-color);
            padding-bottom: 10px;
            margin-bottom: 20px;
        }

        nav ul {
            list-style: none;
            padding: 0;
        }

        nav li {
            margin-bottom: 15px;
        }

        nav a {
            color: #ccc;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
            display: block;
            padding: 5px 0;
        }

        nav a:hover {
            color: white;
            padding-left: 5px;
        }

        /* Main Content */
        main {
            margin-left: var(--sidebar-width);
            padding: 4rem;
            max-width: 1000px;
            width: 100%;
        }

        header {
            margin-bottom: 4rem;
            border-bottom: 1px solid #ddd;
            padding-bottom: 2rem;
        }

        h1 {
            font-size: 2.5rem;
            color: var(--secondary-color);
            margin-bottom: 0.5rem;
        }

        h2 {
            color: var(--primary-color);
            margin-top: 3rem;
            font-size: 1.8rem;
            border-left: 5px solid var(--primary-color);
            padding-left: 15px;
        }

        h3 {
            color: var(--secondary-color);
            margin-top: 1.5rem;
            font-weight: 600;
        }

        p {
            margin-bottom: 1rem;
            text-align: justify;
        }

        ul {
            margin-bottom: 1rem;
        }

        code {
            background-color: #e9ecef;
            padding: 2px 4px;
            border-radius: 4px;
            color: #d63384;
            font-family: monospace;
        }

        .team-members {
            font-size: 1.1rem;
            color: #666;
            font-style: italic;
        }

        /* Callout Boxes for Code Files */
        .code-box {
            background-color: #fff;
            border: 1px solid #ddd;
            border-left: 4px solid #28a745;
            padding: 1rem;
            margin: 1rem 0;
            font-family: monospace;
            font-size: 0.9rem;
        }
        .code-title {
            font-weight: bold;
            color: #28a745;
            margin-bottom: 5px;
            display: block;
        }

        /* Comparison Table */
        table {
            width: 100%;
            border-collapse: separate;
            border-spacing: 0;
            margin: 24px 0;
            background: #fff;
            border-radius: 12px;
            box-shadow: 0 18px 40px rgba(0, 0, 0, 0.12);
            overflow: hidden;
        }
        th, td {
            padding: 14px 16px;
            text-align: left;
            border-bottom: 1px solid #e5e7eb;
            color: #222224;
            font-size: 0.98rem;
        }
        th {
            background: linear-gradient(90deg, #b80f0f, #ee1515);
            color: #fff;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            font-size: 0.85rem;
            border-bottom: none;
        }
        tbody tr:nth-child(even) td {
            background: #f9f9fb;
        }
        tbody tr:hover td {
            background: #fff1f1;
            transition: background 0.2s ease;
        }
        tbody tr:last-child td {
            border-bottom: none;
        }

        /* Image Placeholders */
        .figure {
            background-color: white;
            border: 1px solid #ddd;
            padding: 10px;
            margin: 20px 0;
            text-align: center;
        }

        .figure img {
            width: 80%; /* default figure image width */
            max-width: 100%;
            height: auto;
            background-color: #eee;
            min-height: 200px;
            display: block;
            margin: 0 auto;
        }

        /* Use this class on specific figures you want smaller */
        .figure.figure-small img {
            width: 60%;
            max-width: 600px;
            min-height: 0;
        }


        .caption {
            margin-top: 10px;
            font-size: 0.9rem;
            color: #666;
            font-style: italic;
        }

        footer {
            margin-top: 4rem;
            font-size: 0.9rem;
            color: #888;
            border-top: 1px solid #ddd;
            padding-top: 1rem;
        }
    </style>
</head>
<body>

    <nav>
        <h3>Project Navigation</h3>
        <ul>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#pipeline">The Pipeline</a></li>
            <li><a href="#methods">Methodologies</a></li>
            <li><a href="#code">System Architecture</a></li>
            <li><a href="#comparison">Results</a></li>
            <li><a href="#discussion">Discussions & Future Work</a></li>
            <li><a href="#refs">References</a></li>
        </ul>
    </nav>

    <main>
        <header id="overview">
            <h1>Pokémon Fusion Art Generator</h1>
            <div class="team-members">
                Team Members: Tony Guo, Zhiyi Lai, Hua Zhou
            </div>
            <p><strong>CS 566 Course Project | Fall 2025</strong></p>
            <p>
                This project implements a two-stage image generation pipeline for creating novel Pokémon art.
                We start from low-fidelity sprite fusions and then “lift” them into high-fidelity illustrations
                using modern deep learning based style-transfer and generative models.
            </p>

            <h3>Motivation</h3>
            <p>
                Fan-made Pokémon fusions are everywhere online, but most tools work by literally
                copy-pasting sprite parts: cut the head from one Pokémon, glue it onto the body
                of another, maybe recolor a few pixels by hand, and call it done. The results are
                often charming but visually inconsistent - color palettes clash, shading directions
                disagree, and silhouettes can look unintentionally “broken.”
            </p>
            <p>
                At the same time, state-of-the-art generative models such as Stable Diffusion can
                produce stunning anime-style monsters, but they know nothing about the underlying
                sprite logic that fans care about (which parts came from which parent, what counts
                as a “valid” Pokémon pose, etc.). There is a gap between <em>structure-aware</em>
                fusion tools and <em>style-rich</em> generative models.
            </p>
            <p>
                Our project aims to bridge this gap. We first build a deterministic, rule-based
                fusion algorithm that respects sprite composition and color harmony, and then
                systematically explore three deep learning approaches (VGG19 NST, CycleGAN,
                and Stable Diffusion) to see which ones can lift those fusions into high-fidelity
                art without losing the underlying Pokémon-like structure.
            </p>
            <p>
                Our goal is to build a system that respects both sides: 
                <strong>(1)</strong> preserve the structural logic of sprite fusions and 
                <strong>(2)</strong> leverage deep models to generate polished, high-resolution art.
            </p>

            <h3>Research Questions</h3>
            <ul>
                <li>How far can rule-based pixel fusion go in producing coherent sprite fusions?</li>
                <li>How well do different style transfer paradigms (VGG19 NST, CycleGAN, Stable Diffusion) handle pixel art inputs?</li>
                <li>What are the trade-offs between structure preservation and artistic richness?</li>
            </ul>
        </header>

        <section id="pipeline">
            <h2>The Core Pipeline</h2>
            <p>
                Our system operates in two major stages. Stage 1 ensures that the fused sprite is structurally
                and chromatically coherent. Stage 2 then applies one of three deep learning methods to convert
                this low-resolution fusion into a high-fidelity illustration.
            </p>

            <h3>Stage 1: Coherent Pixel Fusion</h3>
            <p>
                Before applying AI models, we generate a structurally sound base image from two input Pokémon sprites.
                This stage is fully deterministic: the same pair (and seed) always produces the same fusion.
                The fusion algorithm performs:
            </p>
            <ul>
                <li>
                    <strong>Part-based composition:</strong> 
                    Each input sprite is segmented into logical parts (e.g., head, body, limbs, accessories).
                    We place these parts using simple anchor points (such as head center and torso center)
                    to avoid jarring overlaps or gaps.
                </li>
                <li>
                    <strong>Palette harmonization:</strong> 
                    We extract the top-K colors from both sprites and merge them in a perceptual color space
                    (e.g., OKLCH/CIELAB). We then adjust saturation and contrast to avoid extremely clashing
                    combinations such as neon green against very dark red.
                </li>
                <li>
                    <strong>Seam smoothing & shading consistency:</strong> 
                    Where two parts meet, we use edge-aware feathering to soften seams. We also enforce a
                    single, consistent “light direction” for the sprite to avoid conflicting highlights and shadows.
                </li>
                <li>
                    <strong>Stable Diffusion:</strong>
                    For Stable Diffusion, we do not train anything from scratch. Instead, we use a pre-trained
                    image-to-image pipeline and combine the fused sprite with text prompts such as 
                    <em>“anime style, detailed, 3D render”</em>.
                </li>
            </ul>

            <h3>Method: Stable Diffusion (Latent Diffusion)</h3>
            <p>
                Our final method uses a pre-trained Latent Diffusion Model, <strong>Stable Diffusion</strong>, in
                image-to-image mode. Rather than optimizing pixels from scratch or training a GAN, we treat the fused
                sprite as a noisy “hint” and let the model generate an image consistent with both the sprite and
                a text prompt.
            </p>
            <ul>
                <li>
                    <strong>Configuration:</strong> 
                    Using <code>StableDiffusionImg2ImgPipeline</code>, we control two key hyperparameters:
                    <em>strength</em> (how much to deviate from the input sprite) and
                    <em>guidance scale</em> (how strongly the text prompt steers the image).
                </li>
                <li>
                    <strong>CLI & backend usage:</strong> 
                    The script <code>ai_fusion.py</code> allows us to run fusions from the command line
                    (two image paths in, one fusion image out), while <code>fusion_server.py</code> exposes a Flask API 
                    so that the web UI can send sprite pairs and get back rendered fusions.
                </li>
                <li>
                    <strong>Observed behavior:</strong> 
                    Stable Diffusion tends to produce the most visually appealing results. It can add shading, volume and
                    texture (fur, scales, reflections) that are not present in the original sprites, while still
                    roughly preserving the fused creature's silhouette.
                </li>
            </ul>

            <p>
                This produces a fused pixel-art sprite that still “reads” as a Pokémon: clear outline, consistent
                palette, and a plausible silhouette. The output of Stage 1 becomes the content input for Stage 2.
            </p>

            <div class="figure">
                <img src="Figure_1.png" alt="High-Level Project Pipeline">
                <div class="caption">
                    Figure 1: High-level pipeline. Stage 1 produces a coherent fused sprite; Stage 2 applies one of three deep-learning methods.
                </div>
            </div>
        </section>

        <section id="methods">
            <h3>Stage 2 Methodologies: A Comparative Study</h3>
            <p>
                In Stage 2, we explore three different families of models to “upscale” the fused sprite into
                higher fidelity art. For each method, we treat the Stage 1 fusion as a content signal and try
                to inject richer style information from other domains.
            </p>

            <h3>Training Data & Experimental Setup</h3>
            <p>
                We use separate datasets and pipelines for the three methods:
            </p>
            <ul>
                <li>
                    <strong>VGG19 Neural Style Transfer:</strong>
                    A folder of fused sprites (<code>content_images/</code>) and a small library of style
                    references (<code>style_images/</code>, e.g., anime art, paintings). We run NST for all
                    style-content combinations to systematically inspect failure modes.
                </li>
                <li>
                    <strong>CycleGAN:</strong>
                    An unpaired dataset with <code>trainA/</code> (pixel sprites) and <code>trainB/</code> 
                    (target digital art style). No one-to-one pairing is assumed, so the model must learn the
                    mapping “sprite → art” only from distribution statistics.
                </li>
            </ul>

            <h3>Dataset & Preprocessing</h3>
            <p>
                For this course project we use Pokémon images as the source domain and several
                art styles as target domains. For CycleGAN, the dataset is organized into a
                standard unpaired image-to-image layout under <code>dataset/</code>: 
                <code>dataset/trainA</code> contains Pokémon images (content domain) and 
                <code>dataset/trainB</code> contains artworks in the target style. The three
                styles we experimented with are Studio Ghibli anime art, Pointillism paintings,
                and Ukiyo-e woodblock prints, each stored in its own project folder with a
                separate <code>model/</code>, <code>content/</code>, and <code>result/</code>
                directory structure.
            </p>
            <p>
                For VGG19 Neural Style Transfer, we follow a simpler directory layout:
                <code>content_images/</code> stores input photos or fused sprites,
                <code>style_images/</code> stores the reference artworks, and
                <code>output_images/</code> contains all generated results. The script
                <code>style_transfer.py</code> iterates over every combination of content
                and style image and writes the stylized outputs into the corresponding
                subfolders.
            </p>
            <p>
                Across all methods, we resize images to a fixed square resolution (e.g.,
                256&nbsp;x&nbsp;256), composite transparent PNG sprites onto a solid
                background color, and normalize pixel values to the ranges expected by each
                model (ImageNet mean/std for VGG19, <code>[-1, 1]</code> for CycleGAN, and
                <code>[0, 1]</code> for Stable Diffusion). All datasets are used strictly
                for CS&nbsp;566 coursework and are not redistributed.
            </p>

            <h3>Method A: VGG19 Neural Style Transfer</h3>
            <p>
                Our baseline approach uses a pre-trained VGG19 network and optimizes the pixels of an input image directly.
                We follow the classic NST formulation: the fused sprite provides <em>content</em> features, while separate
                artworks provide <em>style</em> features.
            </p>
            <ul>
                <li>
                    <strong>Loss Functions:</strong> 
                    We combine content loss (MSE on feature activations), style loss (MSE on Gram matrices)
                    and a <code>TVLoss</code> term to encourage spatial smoothness.
                </li>
                <li>
                    <strong>Implementation details:</strong> 
                    The script <code>style_transfer.py</code> batch-processes all 
                    <code>(style, content)</code> pairs, runs ~300 optimization steps per image, and saves
                    outputs to a style-specific folder (e.g., <code>output_images/opt_vgg19_star_anime/...</code>).
                </li>
                <li>
                    <strong>Observed behavior:</strong> 
                    On photographic content, the method works as expected. On pixel sprites, however, the model often
                    produces “dirty” textures, washed-out outlines, and artifacts around what used to be flat color regions.
                </li>
            </ul>

            <h3>Method B: CycleGAN (Unpaired Translation)</h3>
            <p>
                To avoid hand-crafting a style loss, we train a CycleGAN to learn the mapping between 
                <em>“pixel sprite domain”</em> and <em>“digital art domain”</em> in an unpaired fashion.
                The generator learns to redraw sprite-like content using the target art style.
            </p>
            <ul>
                <li>
                    <strong>Architecture:</strong> 
                    We use a ResNet-based generator (<code>GeneratorResNet</code>) with 9 residual blocks and a
                    PatchGAN discriminator (<code>Discriminator</code>) as implemented in <code>cyclegan_train.py</code>.
                </li>
                <li>
                    <strong>Losses:</strong> 
                    Standard GAN loss, cycle consistency loss (to ensure that A → B → A returns a sprite-like image), and
                    identity loss (to keep colors stable when images are already in the target domain).
                </li>
                <li>
                    <strong>Training loop:</strong> 
                    The training script uses replay buffers to stabilize discriminator updates and periodically checkpoints
                    the generator weights (e.g., <code>model/G_AB_10.pth</code>, <code>model/G_AB_20.pth</code>, …).
                    The script <code>cyclegan_gen.py</code> then runs batch inference over all saved checkpoints to evaluate
                    how the style evolves over training.
                </li>
                <li>
                    <strong>Observed behavior:</strong> 
                    CycleGAN does a better job than VGG19 at preserving sprite outlines, but sometimes hallucinates
                    repetitive patterns or over-smooths small details like eyes and fingers.
                </li>
            </ul>
        </section>

        <section id="code">
            <h2>System Architecture & Code Structure</h2>
            <p>
                The project is organized into modular Python scripts, separating offline experiments from the
                interactive web demo.
            </p>

            <h3>Offline Experimentation Scripts</h3>

            <div class="code-box">
                <span class="code-title">style_transfer.py</span>
                Batch Neural Style Transfer with VGG19. Implements custom <code>ContentLoss</code>,
                <code>StyleLoss</code>, and <code>TVLoss</code> modules, and loops over all content–style
                combinations to generate grids of NST results.
            </div>

            <div class="code-box">
                <span class="code-title">cyclegan_train.py</span>
                Full CycleGAN training loop. Contains <code>GeneratorResNet</code>, 
                <code>Discriminator</code>, the unpaired <code>ImageDataset</code>, and a 
                <code>ReplayBuffer</code> for stabilizing adversarial training. Saves generator
                checkpoints to the <code>model/</code> directory.
            </div>

            <div class="code-box">
                <span class="code-title">cyclegan_gen.py</span>
                Loads each saved <code>G_AB_*.pth</code> model in turn and runs batch inference over
                input sprites in <code>content/</code>. Outputs are grouped by epoch to compare how
                the generator's style changes throughout training.
            </div>

            <div class="code-box">
                <span class="code-title">ai_fusion.py</span>
                Command-line Stable Diffusion image-to-image fusion. Accepts two input images and
                parameters like <code>--strength</code> and <code>--guidance</code>, then saves a
                high-fidelity fusion image to disk.
            </div>

            <h3>Web Backend</h3>

            <div class="code-box">
                <span class="code-title">fusion_server.py</span>
                Flask-based backend for the interactive demo. Loads the Stable Diffusion pipeline once
                at startup, exposes <code>/api/samples</code> for listing gallery sprites and
                <code>/api/fuse</code> for generating fusions from either sample sprites or user uploads.
            </div>

            <p>
                On top of this backend, a simple JavaScript front-end allows users to pick Pokémon A/B,
                adjust prompts and hyperparameters, and view/download the generated fusions in the browser.
            </p>
        </section>

        <section id="comparison">
        <h2>Results</h2>

        <h3>VGG19 NST Results</h3>
        <p>
            When we apply VGG19 Neural Style Transfer to sprite inputs, the model often struggles.
            On high-resolution photographic content, NST produces the expected painterly textures,
            but on our low-resolution Pokémon sprites it tends to inject noisy, “dirty” patterns
            into flat regions and soften important outlines. Even after tuning the style/content
            weights and adding TV loss, many outputs look visually messy rather than polished.
        </p>
        <div class="figure figure-small">
            <img src="Figure_2.png" alt="Example VGG19 NST outputs on Pokémon sprites">
            <div class="caption">
                Figure 2: Example VGG19 NST results on Pokémon sprites. Flat regions pick up noisy texture
                and outlines become softer than in the original sprites.
            </div>
        </div>

        <h3>CycleGAN Results</h3>
        <p>
            CycleGAN performs better at redrawing sprites into the target art domain. It typically preserves
            the rough silhouette and large-scale color layout while adding smoother shading and materials.
            However, training is unstable: some checkpoints produce over-smoothed images, and others exhibit
            strong checkerboard or tiling artifacts, especially in the background and on large flat surfaces.
            These pattern artifacts are a clear visual failure mode of the model.
        </p>
        <div class="figure">
            <img src="Figure_6.png" alt="Example CycleGan results">
            <div class="caption">
                Figure 3: Example CycleGan results. This figure demonstrates the application of a CycleGAN model
                 to transform the artistic style of various Pokémon.
            </div>
        </div>

        <h3>Stable Diffusion Fusion Results</h3>
        <p>
            Stable Diffusion, used in image-to-image mode, generally produces the most appealing final art.
            Given a fused sprite and an anime-style prompt, it adds consistent lighting, volume and surface
            detail while still roughly respecting the fused creature's pose and proportions. Unlike NST and
            CycleGAN, which are tightly bound to their training distributions, the diffusion model can also
            interpret prompts (e.g., “more menacing”, “soft lighting”) to change the mood of the result.
            The main trade-off is that very high strength values can drift too far from the original fusion.
        </p>
        <div class="figure">
            <img src="Figure_4.png" alt="Example Stable Diffusion fusion outputs">
            <div class="caption">
                Figure 4: Example Stable Diffusion results. The model adds
                shading and detail while keeping the overall creature identity recognizable.
            </div>
        </div>

        <h3>Overall Comparison</h3>
        <p>
            Qualitatively, we can summarize the three methods along four axes: structure preservation,
            texture quality, inference time, and perceived aesthetic quality. VGG19 NST preserves structure
            but introduces heavy noise; CycleGAN can redraw sprites in the target style but sometimes adds
            artifacts; Stable Diffusion offers the best overall visual quality at the cost of more complex
            control over prompts and strength.
        </p>

        <table>
            <thead>
                <tr>
                    <th>Feature</th>
                    <th>VGG19 NST</th>
                    <th>CycleGAN</th>
                    <th>Stable Diffusion</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Structure Preservation</strong></td>
                    <td>High (but rigid and sometimes distorted during optimization)</td>
                    <td>Medium (good outlines but occasional warping)</td>
                    <td>High (can be tuned via strength)</td>
                </tr>
                <tr>
                    <td><strong>Texture Quality</strong></td>
                    <td>Low (noisy, “dirty” textures on flat regions)</td>
                    <td>Medium (stylized but sometimes repetitive patterns)</td>
                    <td>Very High (rich shading, materials, lighting)</td>
                </tr>
                <tr>
                    <td><strong>Inference Time</strong></td>
                    <td>Slow (hundreds of optimization steps per image)</td>
                    <td>Fast (one forward pass after training)</td>
                    <td>Medium (tens of diffusion steps per image)</td>
                </tr>
                <tr>
                    <td><strong>Overall Aesthetic</strong></td>
                    <td>Abstract, often messy</td>
                    <td>Stylized but inconsistent</td>
                    <td>Polished, “finished” illustrations</td>
                </tr>
            </tbody>
        </table>

        <div class="figure">
            <img src="Figure_5.png" alt="Side-by-side comparison of the three methods">
            <div class="caption">
                Figure 5: Side-by-side comparison of VGG19 NST, CycleGAN, and Stable Diffusion
                on the same fused sprite input.
            </div>
        </div>
    </section>

        <section id="discussion">
            <h2>Discussion, Problems Encountered & Future Work</h2>

            <h3>Impact of Fusion Quality on Style Transfer</h3>
                <p>
                    During implementation we also noticed that our Stage&nbsp;1 fused sprites are visibly
                    rougher than the original, hand-crafted Pokémon sprites: edges are less clean and some
                    parts look slightly off-model. When we fed these rough fusions directly into VGG19
                    style transfer, the imperfections were often amplified - noisy backgrounds and
                    broken outlines became even more noticeable after stylization.
                </p>
                <p>
                    To disentangle issues caused by the models from issues caused by our fusion stage,
                    we therefore also ran style transfer experiments on <em>original</em> Pokémon sprites
                    without fusion. These runs generally produced cleaner results, confirming that the
                    quality of the content input is an important bottleneck. In other words, even a
                    strong style-transfer model cannot fully “fix” a low-quality fusion; both stages
                    have to be reasonably good for the final art to look polished.
                </p>
            
            <h3>The Frequency Mismatch Problem</h3>
            <p>
                A recurring issue with VGG19 NST is the mismatch between training data and our inputs. 
                VGG19 was trained on natural photographs, which are rich in high-frequency detail.
                Our sprites, however, are low-resolution and dominated by large, flat regions of solid color.
                As a result, the style loss encourages the model to inject artificial noise where no meaningful
                high-level features exist, breaking the clean pixel-art aesthetic.
            </p>

            <h3>GAN Training Instability</h3>
            <p>
                CycleGAN training surfaces classic GAN issues: mode collapse, sensitivity to hyperparameters,
                and the need for a reasonably large and diverse dataset. Even with replay buffers and
                cycle consistency, some epochs produce over-smoothed images or artifacts. We also observed
                that small tweaks to the learning rate or normalization strategy can substantially change the
                behavior of the generators and discriminators.
            </p>

            <div class="figure figure-small">
                <img src="Figure_3.png" alt="CycleGAN pattern artifacts and NST noisy background failure cases">
                <div class="caption">
                    Figure 6: Common Failure Cases. On the left, a CycleGAN-generated image of
                    Bulbasaur exhibits checkerboard pattern artifacts. On the right, an NST-generated 
                    image of Charmander is overwhelmed by a noisy, grainy background.
                </div>
            </div>

            <h3>Practical Challenges</h3>
            <ul>
                <li><strong>Compute & memory:</strong> 
                    Running VGG19 NST and Stable Diffusion at reasonably high resolutions required careful
                    management of batch sizes and image dimensions to fit within GPU/CPU memory limits.
                </li>
                <li><strong>Data preparation:</strong> 
                    Handling transparent PNGs correctly (especially for sprites) was non-trivial. We had to
                    standardize backgrounds and resolutions across all scripts.
                </li>
                <li><strong>Licensing concerns:</strong> 
                    For a public-facing demo, we would need to replace copyrighted Pokémon sprites with either
                    original assets or properly licensed “Pokémon-like” creatures.
                </li>
            </ul>

            <h3>The Superiority of Latent Diffusion (For This Task)</h3>
            <p>
                Overall, Stable Diffusion provided the most compelling results. Its latent-space representation,
                combined with text prompts, lets it act as a semantic upscaler: it understands what the fused sprite
                is <em>supposed</em> to be (e.g., a dragon-like creature with wings and fire) and fills in plausible
                details that are impossible to obtain with fixed VGG19 features or an unpaired CycleGAN alone.
            </p>

            <h3>Key Findings</h3>
            <ul>
                <li>Rule-based Stage 1 fusion plus palette harmonization already removes most obvious
                    color clashes and produces coherent fused sprites.</li>
                <li>Standard VGG19 NST is poorly matched to low-resolution sprite inputs and tends
                    to create noisy or “dirty” textures.</li>
                <li>CycleGAN can learn a sprite→art mapping, but training is unstable and sometimes
                    introduces checkerboard or pattern artifacts.</li>
                <li>Stable Diffusion, even without fine-tuning, gives the most polished results by
                    acting as a semantic upscaler guided by text prompts.</li>
            </ul>


            <h3>Future Work</h3>
            <ul>
                <li>
                    <strong>Better sprite-aware models:</strong> 
                    Explore fine-tuning diffusion models on sprite-style art so that the outputs preserve more of the
                    pixel-art aesthetic instead of always pushing toward “3D render” realism.
                </li>
                <li>
                    <strong>Interactive user controls:</strong> 
                    Let users mix styles (e.g., 50% watercolor, 50% anime), or lock certain regions (like the face)
                    to preserve key features from the original sprites.
                </li>
                <li>
                    <strong>User study:</strong> 
                    Run a small user study comparing raw sprite fusions, CycleGAN outputs, and Stable Diffusion
                    outputs on perceived quality, recognizability, and “Pokémon-ness.”
                </li>
            </ul>
        </section>

        <section id="refs">
        <h2>References</h2>
            <p>[1] Gatys, Ecker, Bethge. “A Neural Algorithm of Artistic Style.” 2015.</p>
            <p>[2] Zhu et al. “Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.” 2017.</p>
            <p>[3] Rombach et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022.</p>
        </section>


        <footer>
            <p>CS 566 Course Project | Fall 2025</p>
            <p>
                <a href="https://github.com/TonyGuo30/Pokemon-Fusion-Art-Generator" target="_blank" rel="noopener noreferrer">
                    GitHub Link to our project
                </a>
            </p>
            <p>
                <a href="https://drive.google.com/file/d/1zkNtg5PAkDAgcZ56qVN6zbdlx3iSTJ2v/view?usp=sharing" target="_blank" rel="noopener noreferrer">
                    Demo Presentation Link to our project
                </a>
            </p>
        </footer>
    </main>

    <script>
        // Smooth scrolling for sidebar anchors
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>
