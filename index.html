<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pokémon Fusion Art Generator - CS 566 Project</title>
    <style>
        :root {
            --primary-color: #EE1515; /* Pokémon Red */
            --secondary-color: #222224; /* Dark Gray */
            --accent-color: #3B4CCA; /* Pokemon Blue */
            --bg-color: #f8f9fa;
            --text-color: #333;
            --sidebar-width: 280px;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            background-color: var(--bg-color);
            display: flex;
        }

        /* Sidebar Navigation */
        nav {
            width: var(--sidebar-width);
            background-color: var(--secondary-color);
            color: white;
            height: 100vh;
            position: fixed;
            padding: 2rem 1rem;
            box-sizing: border-box;
            overflow-y: auto;
        }

        nav h3 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--primary-color);
            padding-bottom: 10px;
            margin-bottom: 20px;
        }

        nav ul {
            list-style: none;
            padding: 0;
        }

        nav li {
            margin-bottom: 15px;
        }

        nav a {
            color: #ccc;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
            display: block;
            padding: 5px 0;
        }

        nav a:hover {
            color: white;
            padding-left: 5px;
        }

        /* Main Content */
        main {
            margin-left: var(--sidebar-width);
            padding: 4rem;
            max-width: 1000px;
            width: 100%;
        }

        header {
            margin-bottom: 4rem;
            border-bottom: 1px solid #ddd;
            padding-bottom: 2rem;
        }

        h1 {
            font-size: 2.5rem;
            color: var(--secondary-color);
            margin-bottom: 0.5rem;
        }

        h2 {
            color: var(--primary-color);
            margin-top: 3rem;
            font-size: 1.8rem;
            border-left: 5px solid var(--primary-color);
            padding-left: 15px;
        }

        h3 {
            color: var(--secondary-color);
            margin-top: 1.5rem;
            font-weight: 600;
        }

        p {
            margin-bottom: 1rem;
            text-align: justify;
        }

        ul {
            margin-bottom: 1rem;
        }

        code {
            background-color: #e9ecef;
            padding: 2px 4px;
            border-radius: 4px;
            color: #d63384;
            font-family: monospace;
        }

        .team-members {
            font-size: 1.1rem;
            color: #666;
            font-style: italic;
        }

        /* Callout Boxes for Code Files */
        .code-box {
            background-color: #fff;
            border: 1px solid #ddd;
            border-left: 4px solid #28a745;
            padding: 1rem;
            margin: 1rem 0;
            font-family: monospace;
            font-size: 0.9rem;
        }
        .code-title {
            font-weight: bold;
            color: #28a745;
            margin-bottom: 5px;
            display: block;
        }

        /* Comparison Table */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: var(--secondary-color);
            color: white;
        }

        /* Image Placeholders */
        .figure {
            background-color: white;
            border: 1px solid #ddd;
            padding: 10px;
            margin: 20px 0;
            text-align: center;
        }

        .figure img {
            max-width: 100%;
            height: auto;
            background-color: #eee;
            min-height: 200px;
            display: block;
            margin: 0 auto;
        }

        .caption {
            margin-top: 10px;
            font-size: 0.9rem;
            color: #666;
            font-style: italic;
        }

        footer {
            margin-top: 4rem;
            font-size: 0.9rem;
            color: #888;
            border-top: 1px solid #ddd;
            padding-top: 1rem;
        }
    </style>
</head>
<body>

    <nav>
        <h3>Project Navigation</h3>
        <ul>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#pipeline">The Pipeline</a></li>
            <li><a href="#methods">Methodologies (3)</a></li>
            <li><a href="#code">System Architecture</a></li>
            <li><a href="#comparison">Comparative Results</a></li>
            <li><a href="#discussion">Findings & Future Work</a></li>
        </ul>
    </nav>

    <main>
        <header id="overview">
            <h1>Pokémon Fusion Art Generator</h1>
            <div class="team-members">
                Team Members: Tony Guo, Zhiyi Lai, Hua Zhou
            </div>
            <p><strong>CS 566 Course Project | Fall 2025</strong></p>
            <p>
                This project implements a two-stage image generation pipeline for creating novel Pokémon art.
                We start from low-fidelity sprite fusions and then “lift” them into high-fidelity illustrations
                using modern deep learning based style-transfer and generative models.
            </p>

            <h3>Motivation</h3>
            <p>
                Fan-made Pokémon fusions are popular, but most tools simply copy–paste sprite parts together.
                The results often look visually inconsistent: color palettes clash, shading is broken, and
                silhouettes are awkward. At the same time, state-of-the-art generative models like Stable Diffusion
                can create stunning artwork—but they are rarely connected to carefully designed sprite-level logic.
            </p>
            <p>
                Our goal is to build a system that respects both sides: 
                <strong>(1)</strong> preserve the structural logic of sprite fusions and 
                <strong>(2)</strong> leverage deep models to generate polished, high-resolution art.
            </p>

            <h3>Research Questions</h3>
            <ul>
                <li>How far can rule-based pixel fusion go in producing coherent sprite fusions?</li>
                <li>How well do different style transfer paradigms (VGG19 NST, CycleGAN, Stable Diffusion) handle pixel art inputs?</li>
                <li>What are the trade-offs between structure preservation and artistic richness?</li>
            </ul>
        </header>

        <section id="pipeline">
            <h2>The Core Pipeline</h2>
            <p>
                Our system operates in two major stages. Stage 1 ensures that the fused sprite is structurally
                and chromatically coherent. Stage 2 then applies one of three deep learning methods to convert
                this low-resolution fusion into a high-fidelity illustration.
            </p>

            <h3>Stage 1: Coherent Pixel Fusion</h3>
            <p>
                Before applying AI models, we generate a structurally sound base image from two input Pokémon sprites.
                This stage is fully deterministic: the same pair (and seed) always produces the same fusion.
                The fusion algorithm performs:
            </p>
            <ul>
                <li>
                    <strong>Part-based composition:</strong> 
                    Each input sprite is segmented into logical parts (e.g., head, body, limbs, accessories).
                    We place these parts using simple anchor points (such as head center and torso center)
                    to avoid jarring overlaps or gaps.
                </li>
                <li>
                    <strong>Palette harmonization:</strong> 
                    We extract the top-K colors from both sprites and merge them in a perceptual color space
                    (e.g., OKLCH/CIELAB). We then adjust saturation and contrast to avoid extremely clashing
                    combinations such as neon green against very dark red.
                </li>
                <li>
                    <strong>Seam smoothing & shading consistency:</strong> 
                    Where two parts meet, we use edge-aware feathering to soften seams. We also enforce a
                    single, consistent “light direction” for the sprite to avoid conflicting highlights and shadows.
                </li>
            </ul>
            <p>
                This produces a fused pixel-art sprite that still “reads” as a Pokémon: clear outline, consistent
                palette, and a plausible silhouette. The output of Stage 1 becomes the content input for Stage 2.
            </p>

            <div class="figure">
                <img src="Figure_1.png" alt="High-Level Project Pipeline">
                <div class="caption">
                    Figure 1: High-level pipeline. Stage 1 produces a coherent fused sprite; Stage 2 applies one of three deep-learning methods.
                </div>
            </div>
        </section>

        <section id="methods">
            <h2>Stage 2 Methodologies: A Comparative Study</h2>
            <p>
                In Stage 2, we explore three different families of models to “upscale” the fused sprite into
                higher fidelity art. For each method, we treat the Stage 1 fusion as a content signal and try
                to inject richer style information from other domains.
            </p>

            <h3>Training Data & Experimental Setup</h3>
            <p>
                We use separate datasets and pipelines for the three methods:
            </p>
            <ul>
                <li>
                    <strong>VGG19 Neural Style Transfer:</strong>
                    A folder of fused sprites (<code>content_images/</code>) and a small library of style
                    references (<code>style_images/</code>, e.g., anime art, paintings). We run NST for all
                    style–content combinations to systematically inspect failure modes.
                </li>
                <li>
                    <strong>CycleGAN:</strong>
                    An unpaired dataset with <code>trainA/</code> (pixel sprites) and <code>trainB/</code> 
                    (target digital art style). No one-to-one pairing is assumed, so the model must learn the
                    mapping “sprite → art” only from distribution statistics.
                </li>
                <li>
                    <strong>Stable Diffusion:</strong>
                    For Stable Diffusion, we do not train anything from scratch. Instead, we use a pre-trained
                    image-to-image pipeline and combine the fused sprite with text prompts such as 
                    <em>“anime style, detailed, 3D render”</em>.
                </li>
            </ul>

            <h3>Method A: VGG19 Neural Style Transfer</h3>
            <p>
                Our baseline approach uses a pre-trained VGG19 network and optimizes the pixels of an input image directly.
                We follow the classic NST formulation: the fused sprite provides <em>content</em> features, while separate
                artworks provide <em>style</em> features.
            </p>
            <ul>
                <li>
                    <strong>Loss Functions:</strong> 
                    We combine content loss (MSE on feature activations), style loss (MSE on Gram matrices)
                    and a <code>TVLoss</code> term to encourage spatial smoothness.
                </li>
                <li>
                    <strong>Implementation details:</strong> 
                    The script <code>style_transfer.py</code> batch-processes all 
                    <code>(style, content)</code> pairs, runs ~300 optimization steps per image, and saves
                    outputs to a style-specific folder (e.g., <code>output_images/opt_vgg19_star_anime/...</code>).
                </li>
                <li>
                    <strong>Observed behavior:</strong> 
                    On photographic content, the method works as expected. On pixel sprites, however, the model often
                    produces “dirty” textures, washed-out outlines, and artifacts around what used to be flat color regions.
                </li>
            </ul>

            <h3>Method B: CycleGAN (Unpaired Translation)</h3>
            <p>
                To avoid hand-crafting a style loss, we train a CycleGAN to learn the mapping between 
                <em>“pixel sprite domain”</em> and <em>“digital art domain”</em> in an unpaired fashion.
                The generator learns to redraw sprite-like content using the target art style.
            </p>
            <ul>
                <li>
                    <strong>Architecture:</strong> 
                    We use a ResNet-based generator (<code>GeneratorResNet</code>) with 9 residual blocks and a
                    PatchGAN discriminator (<code>Discriminator</code>) as implemented in <code>cyclegan_train.py</code>.
                </li>
                <li>
                    <strong>Losses:</strong> 
                    Standard GAN loss, cycle consistency loss (to ensure that A → B → A returns a sprite-like image), and
                    identity loss (to keep colors stable when images are already in the target domain).
                </li>
                <li>
                    <strong>Training loop:</strong> 
                    The training script uses replay buffers to stabilize discriminator updates and periodically checkpoints
                    the generator weights (e.g., <code>model/G_AB_10.pth</code>, <code>model/G_AB_20.pth</code>, …).
                    The script <code>cyclegan_gen.py</code> then runs batch inference over all saved checkpoints to evaluate
                    how the style evolves over training.
                </li>
                <li>
                    <strong>Observed behavior:</strong> 
                    CycleGAN does a better job than VGG19 at preserving sprite outlines, but sometimes hallucinates
                    repetitive patterns or over-smooths small details like eyes and fingers.
                </li>
            </ul>

            <h3>Method C: Stable Diffusion (Latent Diffusion)</h3>
            <p>
                Our final method uses a pre-trained Latent Diffusion Model, <strong>Stable Diffusion</strong>, in
                image-to-image mode. Rather than optimizing pixels from scratch or training a GAN, we treat the fused
                sprite as a noisy “hint” and let the model generate an image consistent with both the sprite and
                a text prompt.
            </p>
            <ul>
                <li>
                    <strong>Configuration:</strong> 
                    Using <code>StableDiffusionImg2ImgPipeline</code>, we control two key hyperparameters:
                    <em>strength</em> (how much to deviate from the input sprite) and
                    <em>guidance scale</em> (how strongly the text prompt steers the image).
                </li>
                <li>
                    <strong>CLI & backend usage:</strong> 
                    The script <code>ai_fusion.py</code> allows us to run fusions from the command line
                    (two image paths in, one fusion image out), while <code>fusion_server.py</code> exposes a Flask API 
                    so that the web UI can send sprite pairs and get back rendered fusions.
                </li>
                <li>
                    <strong>Observed behavior:</strong> 
                    Stable Diffusion tends to produce the most visually appealing results. It can add shading, volume and
                    texture (fur, scales, reflections) that are not present in the original sprites, while still
                    roughly preserving the fused creature’s silhouette.
                </li>
            </ul>
        </section>

        <section id="code">
            <h2>System Architecture & Code Structure</h2>
            <p>
                The project is organized into modular Python scripts, separating offline experiments from the
                interactive web demo.
            </p>

            <h3>Offline Experimentation Scripts</h3>

            <div class="code-box">
                <span class="code-title">style_transfer.py</span>
                Batch Neural Style Transfer with VGG19. Implements custom <code>ContentLoss</code>,
                <code>StyleLoss</code>, and <code>TVLoss</code> modules, and loops over all content–style
                combinations to generate grids of NST results.
            </div>

            <div class="code-box">
                <span class="code-title">cyclegan_train.py</span>
                Full CycleGAN training loop. Contains <code>GeneratorResNet</code>, 
                <code>Discriminator</code>, the unpaired <code>ImageDataset</code>, and a 
                <code>ReplayBuffer</code> for stabilizing adversarial training. Saves generator
                checkpoints to the <code>model/</code> directory.
            </div>

            <div class="code-box">
                <span class="code-title">cyclegan_gen.py</span>
                Loads each saved <code>G_AB_*.pth</code> model in turn and runs batch inference over
                input sprites in <code>content/</code>. Outputs are grouped by epoch to compare how
                the generator’s style changes throughout training.
            </div>

            <div class="code-box">
                <span class="code-title">ai_fusion.py</span>
                Command-line Stable Diffusion image-to-image fusion. Accepts two input images and
                parameters like <code>--strength</code> and <code>--guidance</code>, then saves a
                high-fidelity fusion image to disk.
            </div>

            <h3>Web Backend</h3>

            <div class="code-box">
                <span class="code-title">fusion_server.py</span>
                Flask-based backend for the interactive demo. Loads the Stable Diffusion pipeline once
                at startup, exposes <code>/api/samples</code> for listing gallery sprites and
                <code>/api/fuse</code> for generating fusions from either sample sprites or user uploads.
            </div>

            <p>
                On top of this backend, a simple JavaScript front-end allows users to pick Pokémon A/B,
                adjust prompts and hyperparameters, and view/download the generated fusions in the browser.
            </p>
        </section>

        <section id="comparison">
            <h2>Comparative Results</h2>
            <p>
                We qualitatively compare the three methods along four dimensions: structure preservation,
                texture quality, inference time, and overall aesthetic. The fused sprites from Stage 1 are
                used as the common input across all methods.
            </p>
            
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>VGG19 NST</th>
                        <th>CycleGAN</th>
                        <th>Stable Diffusion</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Structure Preservation</strong></td>
                        <td>High (but rigid and sometimes distorted during optimization)</td>
                        <td>Medium (good outlines but occasional warping)</td>
                        <td>High (can be tuned via strength)</td>
                    </tr>
                    <tr>
                        <td><strong>Texture Quality</strong></td>
                        <td>Low (noisy, “dirty” textures on flat regions)</td>
                        <td>Medium (stylized but sometimes repetitive patterns)</td>
                        <td>Very High (rich shading, materials, lighting)</td>
                    </tr>
                    <tr>
                        <td><strong>Inference Time</strong></td>
                        <td>Slow (hundreds of optimization steps per image)</td>
                        <td>Fast (one forward pass after training)</td>
                        <td>Medium (tens of diffusion steps per image)</td>
                    </tr>
                    <tr>
                        <td><strong>Overall Aesthetic</strong></td>
                        <td>Abstract, often messy</td>
                        <td>Stylized but inconsistent</td>
                        <td>Polished, “finished” illustrations</td>
                    </tr>
                </tbody>
            </table>

            <div class="figure">
                <img src="https://via.placeholder.com/800x400?text=Comparison:+VGG19+vs+CycleGAN+vs+Stable+Diffusion" alt="Visual Comparison">
                <div class="caption">
                    Figure 2: Example fusions from the three methods on the same Stage 1 sprite input.
                </div>
            </div>
        </section>

        <section id="discussion">
            <h2>Discussion, Problems Encountered & Future Work</h2>
            
            <h3>The Frequency Mismatch Problem</h3>
            <p>
                A recurring issue with VGG19 NST is the mismatch between training data and our inputs. 
                VGG19 was trained on natural photographs, which are rich in high-frequency detail.
                Our sprites, however, are low-resolution and dominated by large, flat regions of solid color.
                As a result, the style loss encourages the model to inject artificial noise where no meaningful
                high-level features exist, breaking the clean pixel-art aesthetic.
            </p>

            <h3>GAN Training Instability</h3>
            <p>
                CycleGAN training surfaces classic GAN issues: mode collapse, sensitivity to hyperparameters,
                and the need for a reasonably large and diverse dataset. Even with replay buffers and
                cycle consistency, some epochs produce over-smoothed images or artifacts. We also observed
                that small tweaks to the learning rate or normalization strategy can substantially change the
                behavior of the generators and discriminators.
            </p>

            <h3>Practical Challenges</h3>
            <ul>
                <li><strong>Compute & memory:</strong> 
                    Running VGG19 NST and Stable Diffusion at reasonably high resolutions required careful
                    management of batch sizes and image dimensions to fit within GPU/CPU memory limits.
                </li>
                <li><strong>Data preparation:</strong> 
                    Handling transparent PNGs correctly (especially for sprites) was non-trivial. We had to
                    standardize backgrounds and resolutions across all scripts.
                </li>
                <li><strong>Licensing concerns:</strong> 
                    For a public-facing demo, we would need to replace copyrighted Pokémon sprites with either
                    original assets or properly licensed “Pokémon-like” creatures.
                </li>
            </ul>

            <h3>The Superiority of Latent Diffusion (For This Task)</h3>
            <p>
                Overall, Stable Diffusion provided the most compelling results. Its latent-space representation,
                combined with text prompts, lets it act as a semantic upscaler: it understands what the fused sprite
                is <em>supposed</em> to be (e.g., a dragon-like creature with wings and fire) and fills in plausible
                details that are impossible to obtain with fixed VGG19 features or an unpaired CycleGAN alone.
            </p>

            <h3>Future Work</h3>
            <ul>
                <li>
                    <strong>Better sprite-aware models:</strong> 
                    Explore fine-tuning diffusion models on sprite-style art so that the outputs preserve more of the
                    pixel-art aesthetic instead of always pushing toward “3D render” realism.
                </li>
                <li>
                    <strong>Interactive user controls:</strong> 
                    Let users mix styles (e.g., 50% watercolor, 50% anime), or lock certain regions (like the face)
                    to preserve key features from the original sprites.
                </li>
                <li>
                    <strong>User study:</strong> 
                    Run a small user study comparing raw sprite fusions, CycleGAN outputs, and Stable Diffusion
                    outputs on perceived quality, recognizability, and “Pokémon-ness.”
                </li>
            </ul>
        </section>

        <footer>
            <p>CS 566 Course Project | Fall 2025</p>
            <p>
                <a href="https://github.com/TonyGuo30/Pokemon-Fusion-Art-Generator" target="_blank" rel="noopener noreferrer">
                    GitHub Link to our project
                </a>
            </p>
        </footer>
    </main>

    <script>
        // Smooth scrolling for sidebar anchors
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>
